{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch with Examples\n",
    "\n",
    "本教程通过自包含的示例介绍 PyTorch 的基本概念。PyTorch 的核心有两个主要功能：\n",
    "\n",
    "+ n 维张量，类似于 numpy，但可以在 GPU 上运行；\n",
    "+ 用于构建和训练神经网络的自动微分\n",
    "\n",
    "我们将使用一个三阶多项式拟合 $y=sin(x)$ 的问题作为运行示例。网络将有四个参数，并将使用梯度下降法进行训练，通过最小化网络输出与真实输出之间的欧氏距离来拟合随机数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 您可以浏览本页末尾的[各个示例](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#examples-download)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy\n",
    "\n",
    "在介绍 PyTorch 之前，我们先用 numpy 实现网络。Numpy 提供了一个 n 维数组对象，以及许多用于操作这些数组的函数。Numpy 是一个用于科学计算的通用框架；它对计算图、深度学习或梯度一无所知。不过，通过使用 numpy 操作手动实现网络的前向和后向传递，我们可以轻松地使用 numpy 将三阶多项式拟合为正弦函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1962.3593958622448\n",
      "199 1306.4983338251932\n",
      "299 871.0853692527345\n",
      "399 581.9475262483453\n",
      "499 389.89107737722884\n",
      "599 262.2823083159039\n",
      "699 177.46832437379572\n",
      "799 121.07888866003591\n",
      "899 83.57470032693371\n",
      "999 58.621726760761725\n",
      "1099 42.01306985309979\n",
      "1199 30.953813294272425\n",
      "1299 23.586545005790533\n",
      "1399 18.676488761886517\n",
      "1499 15.402504851043869\n",
      "1599 13.218328367534383\n",
      "1699 11.760416116126226\n",
      "1799 10.786729501989374\n",
      "1899 10.136055955748075\n",
      "1999 9.700969573169461\n",
      "Result: y = -0.014130940290577368 + 0.8309195784189938x + 0.0024378223399859947x^2 + -0.08965753899700578x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 创建随机的输入和输出数据\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# 随机初始化权重\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 前向计算：计算预测 y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 计算和输出损失\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "    \n",
    "    # 反向计算：计算 a,b,c,d 相对于 loss 的梯度值\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # 更新权重\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f\"Result: y = {a} + {b}x + {c}x^2 + {d}x^3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch: Tensors\n",
    "\n",
    "numpy 是一个很棒的框架，但它无法利用 GPU 来加速其数值计算。对于现代深度神经网络，GPU 通常可以提供 50 倍或更高的加速，因此不幸的是，numpy 不足以满足现代深度学习的需求。\n",
    "\n",
    "在这里，我们介绍最基本的 PyTorch 概念：Tensor。PyTorch 张量在概念上与 numpy 数组相同：张量是一个 n 维数组，PyTorch 提供了许多用于操作这些张量的函数。在幕后，Tensor 可以跟踪计算图和梯度，但它们也可用作科学计算的通用工具。\n",
    "\n",
    "与 numpy 不同的是，PyTorch 张量可以利用 GPU 来加速其数值计算。要在 GPU 上运行 PyTorch 张量，您只需指定正确的设备即可。\n",
    "\n",
    "在这里，我们使用 PyTorch 张量将三阶多项式拟合为正弦函数。与上面的 numpy 示例类似，我们需要手动实现通过网络的前向和后向传递："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1721.864501953125\n",
      "199 1185.115234375\n",
      "299 817.46044921875\n",
      "399 565.3399658203125\n",
      "499 392.25030517578125\n",
      "599 273.28314208984375\n",
      "699 191.42315673828125\n",
      "799 135.0342559814453\n",
      "899 96.14817810058594\n",
      "999 69.30343627929688\n",
      "1099 50.751808166503906\n",
      "1199 37.91786193847656\n",
      "1299 29.030498504638672\n",
      "1399 22.87004280090332\n",
      "1499 18.59565544128418\n",
      "1599 15.627120018005371\n",
      "1699 13.563624382019043\n",
      "1799 12.127979278564453\n",
      "1899 11.128321647644043\n",
      "1999 10.43166732788086\n",
      "Result: y = 0.03838726878166199 + 0.8735573887825012x + -0.006622442975640297x^2 + -0.09572239220142365x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # 注释该行以运行在 GPU 上\n",
    "\n",
    "# 创建随机输入和输出\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 随机初始化权重\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 前向计算\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 计算和输出损失\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 反向\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # 使用梯度下降更新权重\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f\"Result: y = {a.item()} + {b.item()}x + {c.item()}x^2 + {d.item()}x^3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动微分 - Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch: Tensor 和 autograd\n",
    "\n",
    "在上述示例中，我们不得不手动实现神经网络的前向和反向传递。手动实现反向传递对于小型双层网络来说不算什么，但对于大型复杂网络来说，很快就会变得非常棘手。\n",
    "\n",
    "值得庆幸的是，我们可以使用[自动微分](https://en.wikipedia.org/wiki/Automatic_differentiation)来自动计算神经网络的反向传递。PyTorch 中的 `autograd` 软件包正是提供了这种功能。使用 `autograd` 时，网络的前向传递将定义一个**计算图**；图中的**节点**将是张量，**边**将是通过输入张量产生输出张量的函数。通过该图的**反向传播**，可以轻松计算梯度。\n",
    "\n",
    "这听起来很复杂，但实际使用起来却非常简单。每个张量代表计算图中的一个节点。如果 `x` 是一个 `x.requires_grad=True` 的张量，那么 `x.grad` 就是另一个张量，包含 `x` 相对于某个标量值的梯度。在这里，我们使用 PyTorch 张量和 `autograd` 来实现正弦与三阶多项式拟合的示例；现在我们不再需要手动实现网络的后向传递："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ROCT-Thunk-Interface/src/hymgr.c:309, WARNING]: Version mismatch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "99 1412.75537109375\n",
      "199 943.9651489257812\n",
      "299 631.9915771484375\n",
      "399 424.2935791015625\n",
      "499 285.95855712890625\n",
      "599 193.78067016601562\n",
      "699 132.3299560546875\n",
      "799 91.34317779541016\n",
      "899 63.991355895996094\n",
      "999 45.728450775146484\n",
      "1099 33.527069091796875\n",
      "1199 25.37053680419922\n",
      "1299 19.91449546813965\n",
      "1399 16.262409210205078\n",
      "1499 13.816181182861328\n",
      "1599 12.17640209197998\n",
      "1699 11.076427459716797\n",
      "1799 10.337980270385742\n",
      "1899 9.841805458068848\n",
      "1999 9.508166313171387\n",
      "Result: y = -0.01516183651983738 + 0.8353146910667419x + 0.002615669509395957x^2 + -0.09028270840644836x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# 让模型在加速器上进行训练，例如：CUDA, MPS, MTIA\n",
    "# 如果当前的加速器可以使用，那么就使用它；否则，使用 CPU\n",
    "dtype = torch.float\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# 创建张量保存输入和输出\n",
    "# 默认情况下，requires_grad=False，也就是表明在反向传播过程中，\n",
    "# 我们不需要计算这些张量对应的梯度\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 创建权重的随机值\n",
    "# 对于三阶多项式，我们需要四个权重：y = a + b x + c x^2 + d x^3\n",
    "# 设置 requires_grad=True 以为只我们想要在反向传播过程中，\n",
    "# 计算相应张量的梯度值\n",
    "a = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 前向计算：在输入上使用操作计算预测值 y\n",
    "    y_pred = a + b * x + c * x**2 + d * x**3\n",
    "\n",
    "    # 计算并输出损失值\n",
    "    # 损失值为一个形状为 (1,) 的张量\n",
    "    # loss.item() 获取损失的标量值\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 使用自动梯度计算反向传播过程\n",
    "    # 该调用会计算损失值相对于所有 requires_grad=True 的张量\n",
    "    # 该操作以后，a.grad, b.grad, c.grad, d.grad 为分别保存着\n",
    "    # 损失值相对于 a,b,c,d 的梯度张量值\n",
    "    loss.backward()\n",
    "\n",
    "    # 手动使用梯度下降更新权重，之所以用 torch.no_grad() 进行包围\n",
    "    # 因为 requires_grad=True 的权重，但是在自动微分过程中，我们\n",
    "    # 不需要跟踪梯度值\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # 在更新完权重以后，把所有梯度值手动设置为 0\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "\n",
    "print(f\"Result: y = {a.item()} + {b.item()}x + {c.item()}x^2 + {d.item()}x^3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch: 定义新的自动梯度函数\n",
    "\n",
    "在底层，每个原始自动求导运算符实际上是两个对张量进行操作的函数。前向函数根据输入张量计算输出张量。后向函数接收输出张量相对于某个标量值的梯度，并计算输入张量相对于同一标量值的梯度。\n",
    "\n",
    "在 PyTorch 中，我们可以通过定义 `torch.autograd.Function` 的子类并实现前向和反向函数来轻松定义我们自己的自动求导运算符。然后，我们可以通过构造一个实例并像函数一样调用它来使用我们的新自动求导运算符，传递包含输入数据的张量。\n",
    "\n",
    "在这个例子中，我们定义我们的模型为 $y=a+bP_3(c+dx)$，而不是$y=a+bx+cx^2+dx^3$，其中$P_3(x)=\\frac{1}{2}(5x^3-3x)$是[三次勒让德多项式](https://en.wikipedia.org/wiki/Legendre_polynomials)，我们编写我们自定义的自动微分函数，以计算$P_3$的前向和反向，并使用它来实现我们的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 209.9583282470703\n",
      "199 144.66018676757812\n",
      "299 100.70249938964844\n",
      "399 71.03518676757812\n",
      "499 50.97853088378906\n",
      "599 37.40312957763672\n",
      "699 28.206863403320312\n",
      "799 21.973182678222656\n",
      "899 17.745725631713867\n",
      "999 14.877887725830078\n",
      "1099 12.931764602661133\n",
      "1199 11.610918045043945\n",
      "1299 10.714256286621094\n",
      "1399 10.10548210144043\n",
      "1499 9.69210433959961\n",
      "1599 9.411375045776367\n",
      "1699 9.220745086669922\n",
      "1799 9.091285705566406\n",
      "1899 9.003361701965332\n",
      "1999 8.943639755249023\n",
      "Result: y = 4.1149544882657096e-10 + -2.208526849746704 * P3(-5.391238566687662e-10 + 0.2554861009120941 x)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    通过继承 torch.autograd.Function 我们可以实现自定义自动微分算子\n",
    "    同时在输入张量上实现前向和反向过程\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        在前向过程中，我们会获取到一个包含输入的张量，并返回一个包含输出\n",
    "        的张量。ctx 是一个上下文对象，可用于存储用于反向计算的信息。\n",
    "        您可以使用 ctx.save_for_backward 方法缓存任意对象以供反向传递使用。\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5*(5 * input**3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        在反向传播过程中，我们会获取到一个包含损失相对于输出的梯度张量，\n",
    "        并且需要我们计算损失相对于输入的梯度值。\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "    \n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# 创建保存输入和输出的张量\n",
    "# 默认情况下，requires_grad=False，也就是意味着在反向传播过程中，\n",
    "# 我们不需要计算相对于这些张量的梯度值\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 创建权重的随机张量，在该例子中，我们需要四个权重：y = a + b * P3(c + d * x)\n",
    "# 这些权重需要初始化得离正确结果不太远，以确保收敛。\n",
    "# 设置requires_grad=True 表示我们想要在反向传播过程中计算这些张量的梯度。\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # 为了应用我们的算子，我们使用 Function.apply 方法\n",
    "    # 给方法一个匿名名称：P3\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # 前向过程：使用前向算子计算预测值 y\n",
    "    # 使用自定义自动微分操作计算 P3\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "    # 计算并输出损失\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 使用自动微分计算反向\n",
    "    loss.backward()\n",
    "\n",
    "    # 更新权重\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "    \n",
    "        # 在更新权重以后，手动设置梯度为 0\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn` 模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch: `nn`\n",
    "\n",
    "计算图和自动求导是定义复杂运算符和自动求导的非常强大的范例；然而，对于大型神经网络来说，原始的自动求导可能有点太低级了。\n",
    "\n",
    "在构建神经网络时，我们经常考虑将计算安排到层中，其中一些层具有可学习的参数，这些参数将在学习过程中进行优化。\n",
    "\n",
    "在 TensorFlow 中，[Keras](https://github.com/fchollet/keras)、[TensorFlow-Slim](https://github.com/google-research/tf-slim) 和 [TFLearn](http://tflearn.org/) 等软件包提供了对原始计算图的更高级别的抽象，这对于构建神经网络非常有用。\n",
    "\n",
    "在 PyTorch 中，`nn` 包也用于同样的目的。`nn` 包定义了一组模块，它们大致相当于神经网络层。模块接收输入张量并计算输出张量，但也可能保存内部状态，例如包含可学习参数的张量。`nn` 包还定义了一组有用的损失函数，这些函数在训练神经网络时很常用。\n",
    "\n",
    "在这个例子中，我们使用 `nn` 包来实现我们的多项式模型网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2819.638916015625\n",
      "199 1924.453857421875\n",
      "299 1315.8544921875\n",
      "399 901.6302490234375\n",
      "499 619.3822021484375\n",
      "599 426.8427429199219\n",
      "699 295.3485107421875\n",
      "799 205.44180297851562\n",
      "899 143.8990020751953\n",
      "999 101.72352600097656\n",
      "1099 72.78728485107422\n",
      "1199 52.91194152832031\n",
      "1299 39.24473571777344\n",
      "1399 29.836050033569336\n",
      "1499 23.35184097290039\n",
      "1599 18.878271102905273\n",
      "1699 15.788518905639648\n",
      "1799 13.652347564697266\n",
      "1899 12.1738920211792\n",
      "1999 11.149667739868164\n",
      "Result: y = 0.043910812586545944 + 0.8327017426490784 x + -0.007575344294309616 x^2 + -0.08991103619337082 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        在构造函数中，我们实例化四个参数，并将他们赋值为成员参数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        在前向算子中，我们接受一个输入张量，并必须返回一个输出张量。\n",
    "        我们可以使用构造函数中定义的模块以及张量上的任意运算符。\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "    \n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        就像 Python 中的任何类一样，你也可以在 PyTorch 模块上定义自定义方法\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "    \n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 通过实例化类，构造我们的模型\n",
    "model = Polynomial3()\n",
    "\n",
    "# 构建我们的损失函数和优化器。SGD 构造函数中对 model.parameters() 的调用\n",
    "# 将包含可学习参数（使用 torch.nn.Parameter 定义）它们是模型的成员。\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "\n",
    "for t in range(2000):\n",
    "    # 前向\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 计算损失并输出\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 设置梯度为 0，执行反向传递，并更新权重。\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch：控制流和权重共享\n",
    "\n",
    "作为动态计算图和权重共享的一个例子，我们实现了一个非常奇怪的模型：一个三阶五阶多项式，在每次前向传递时选择一个 3 到 5 之间的随机数并使用那么多阶，多次重复使用相同的权重来计算四阶和五阶。\n",
    "\n",
    "对于这个模型，我们可以使用普通的 Python 流控制来实现循环，我们可以通过在定义前向传递时多次重复使用相同的参数来实现权重共享。\n",
    "\n",
    "我们可以轻松地将此模型实现为 `Module` 子类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 1063.7928466796875\n",
      "3999 475.17193603515625\n",
      "5999 215.9987030029297\n",
      "7999 96.30238342285156\n",
      "9999 48.99087905883789\n",
      "11999 26.618961334228516\n",
      "13999 16.54855728149414\n",
      "15999 12.227919578552246\n",
      "17999 10.307435035705566\n",
      "19999 9.391677856445312\n",
      "21999 9.107739448547363\n",
      "23999 8.944269180297852\n",
      "25999 8.894832611083984\n",
      "27999 8.864897727966309\n",
      "29999 8.853294372558594\n",
      "Result: y = 0.00012930684897582978 + 0.8543539643287659 x + -0.0006214152672328055 x^2 + -0.09329459816217422 x^3 + 0.00011243280459893867 x^4 ? + 0.00011243280459893867 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        在构造函数中，我们实例化五个参数，并将其作为成员\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        对于模型的前向传递，我们随机选择 4 或 5\n",
    "\n",
    "        并重用 e 参数来计算这些顺序的贡献。\n",
    "\n",
    "        由于每个前向传递都会构建一个动态计算图，因此在定义模型的前向传递时，我们可以使用正常的\n",
    "        Python 控制流运算符，如循环或条件语句。\n",
    "\n",
    "        在这里我们还看到，在定义计算图时多次重用相同的参数是完全安全的。\n",
    "        \"\"\"\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        就像 Python 中的任何类一样，你也可以在 PyTorch 模块上定义自定义方法\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 通过实例化上面定义的类来构建我们的模型\n",
    "model = DynamicNet()\n",
    "\n",
    "# 构建我们的损失函数和优化器。\n",
    "# 使用原始随机梯度下降法训练这个奇怪的模型很困难，所以我们使用动量\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
