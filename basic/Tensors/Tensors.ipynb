{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "张量是一种专门的数据结构，与数组和矩阵非常相似。 在 PyTorch 中，我们使用张量来编码模型的输入和输出，以及模型的参数。 张量类似于 [NumPy](https://numpy.org/) 的`ndarrays`，但张量可以在 GPU 或其他硬件加速器上运行。 事实上，Tensors 和 NumPy 数组通常可以共享相同的底层内存，无需复制数据（请参阅与 [NumPy 的桥接](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label)）。 张量还针对自动微分进行了优化（稍后我们将在 [Autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) 部分了解更多相关信息）。 如果你熟悉`ndarrays`，那么使用张量 API 就会得心应手。 如果不熟悉，请继续学习！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化张量\n",
    "\n",
    "张量可以通过多种方式初始化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接从数据初始化\n",
    "\n",
    "张量可以直接从数据创建。数据类型会自动推断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从 Numpy 数组初始化\n",
    "\n",
    "可以从 NumPy 数组创建张量（反之亦然 - 参见使用 [NumPy 的桥接](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从其他 Tensor 初始化\n",
    "\n",
    "除非明确覆盖，否则新的张量将保留参数张量的属性（形状、数据类型）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor:\n",
      " tensor([[0.6614, 0.0480],\n",
      "        [0.0949, 0.8235]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data)\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "print(f\"Random Tensor:\\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机或常量值\n",
    "\n",
    "`shape` 是张量维度的元组。在下面的函数中，它决定了输出张量的维数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor:\n",
      " tensor([[0.0884, 0.2094, 0.1110],\n",
      "        [0.7800, 0.4359, 0.5054]]) \n",
      "\n",
      "One Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zero Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor:\\n {rand_tensor} \\n\")\n",
    "print(f\"One Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zero Tensor: \\n {zeros_tensor} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量的属性\n",
    "\n",
    "张量属性描述它们的形状、数据类型和存储它们的设备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量操作\n",
    "\n",
    "这里全面描述了 100 多个张量运算，包括算术、线性代数、矩阵操作（转置、索引、切片）、采样等。\n",
    "\n",
    "这些操作中的每一个都可以在 GPU 上运行（通常比在 CPU 上运行的速度更快）。如果您使用 Colab，请转到运行时 > 更改运行时类型 > GPU 来分配 GPU。\n",
    "\n",
    "默认情况下，张量是在 CPU 上创建的。我们需要使用 `.to` 方法明确将张量移动到 GPU（在检查 GPU 可用性之后）。请记住，跨设备复制大型张量在时间和内存方面可能会很昂贵！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ROCT-Thunk-Interface/src/hymgr.c:309, WARNING]: Version mismatch\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试列表中的某些操作。如果您熟悉 NumPy API，您会发现 Tensor API 非常容易使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准 numpy 样式索引和切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last colum: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last colum: {tensor[..., -1]}\")\n",
    "tensor[:, 1]=0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 连接张量\n",
    "\n",
    "您可以使用 `torch.cat` 沿给定维度连接一系列张量。另请参阅 `torch.stack`，这是另一个与 `torch.cat` 略有不同的张量连接运算符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0170, 0.5718, 0.9582, 0.1413],\n",
      "         [0.8573, 0.4393, 0.4413, 0.6593],\n",
      "         [0.8606, 0.9637, 0.3154, 0.1142]],\n",
      "\n",
      "        [[0.6672, 0.6707, 0.7186, 0.6208],\n",
      "         [0.1382, 0.1943, 0.4156, 0.5072],\n",
      "         [0.6009, 0.0522, 0.6470, 0.2042]]])\n",
      "tensor([[[0.5100, 0.9052, 0.2818, 0.4196],\n",
      "         [0.9464, 0.2380, 0.3300, 0.1148],\n",
      "         [0.9690, 0.0277, 0.4810, 0.9035],\n",
      "         [0.4706, 0.4664, 0.4136, 0.6139]],\n",
      "\n",
      "        [[0.6039, 0.0271, 0.0154, 0.6438],\n",
      "         [0.5457, 0.0680, 0.8596, 0.7881],\n",
      "         [0.4078, 0.7854, 0.2423, 0.4749],\n",
      "         [0.9580, 0.5843, 0.9059, 0.4782]]])\n",
      "tensor([[[0.0170, 0.5718, 0.9582, 0.1413],\n",
      "         [0.8573, 0.4393, 0.4413, 0.6593],\n",
      "         [0.8606, 0.9637, 0.3154, 0.1142],\n",
      "         [0.5100, 0.9052, 0.2818, 0.4196],\n",
      "         [0.9464, 0.2380, 0.3300, 0.1148],\n",
      "         [0.9690, 0.0277, 0.4810, 0.9035],\n",
      "         [0.4706, 0.4664, 0.4136, 0.6139]],\n",
      "\n",
      "        [[0.6672, 0.6707, 0.7186, 0.6208],\n",
      "         [0.1382, 0.1943, 0.4156, 0.5072],\n",
      "         [0.6009, 0.0522, 0.6470, 0.2042],\n",
      "         [0.6039, 0.0271, 0.0154, 0.6438],\n",
      "         [0.5457, 0.0680, 0.8596, 0.7881],\n",
      "         [0.4078, 0.7854, 0.2423, 0.4749],\n",
      "         [0.9580, 0.5843, 0.9059, 0.4782]]])\n"
     ]
    }
   ],
   "source": [
    "shape1 = (2, 3, 4)\n",
    "shape2 = (2, 4, 4)\n",
    "tensor1 = torch.rand(shape1)\n",
    "tensor2 = torch.rand(shape2)\n",
    "tensor_cat = torch.cat([tensor1, tensor2], dim=1)\n",
    "print(tensor1)\n",
    "print(tensor2)\n",
    "print(tensor_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算术操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matmul: tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "Mul: tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 计算矩阵乘，也就是 y1,y2,y3 拥有相同的值\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "print(f\"Matmul: {y1}\")\n",
    "\n",
    "# 逐元素乘积，z1,z2,z3有相同值\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "print(f\"Mul: {z1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单元素张量\n",
    "\n",
    "如果你有一个单元素张量，例如通过将张量的所有值聚合为一个值，则可以使用 `item()` 将其转换为 Python 数值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 就地运算(In-place Operations)\n",
    "\n",
    "将结果存储到操作数中的运算称为就地运算。它们以 `_` 后缀表示。例如：`x.copy_(y)`、`x.t_()`，将更改 `x`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 就地操作可以节省一些内存，但在计算导数时可能会出现问题，因为会立即丢失历史记录。因此，不鼓励使用它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与 numpy 桥接\n",
    "\n",
    "CPU 和 NumPy 数组上的张量可以共享其底层内存位置，改变其中一个也会改变另一个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor 转为 numpy 数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的变化反映在 NumPy 数组中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy 数组转为 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy 数组的变化反映在张量中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
